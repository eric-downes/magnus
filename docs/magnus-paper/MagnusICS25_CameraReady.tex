\documentclass[sigconf, 10pt, screen]{acmart}

\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{makecell}
\usepackage{titlesec}
\usepackage[subtle]{savetrees}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{breqn}
\usepackage{hyperref}

\def\sectionautorefname{Section}

\graphicspath{{figs/}}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand{\figwidth}{.7\linewidth}
\newcommand{\figwidthFloat}{.35\linewidth}

\def\algorithmautorefname{Algorithm}
\renewcommand{\subsectionautorefname}{Section}
\renewcommand{\subsubsectionautorefname}{Section}

\setlength{\parskip}{0pt}
% %\setlength{\parindent}{0pt}
\titlespacing*{\section}{0pt}{2pt}{2pt}
\titlespacing*{\subsection}{0pt}{2pt}{2pt}
\titlespacing*{\subsubsection}{0pt}{4pt}{4pt}
\setlength{\abovecaptionskip}{3pt}
\setlength{\belowcaptionskip}{2pt}
\setlength{\intextsep}{2pt}
\setlist{nosep}
\setlength{\dbltextfloatsep}{2pt}
\setlength{\textfloatsep}{3pt}
\setlength{\floatsep}{3pt}
\setlength{\dblfloatsep}{2pt}
% \setlist[itemize]{topsep=0pt,partopsep=0pt}
% \setlist[enumerate]{topsep=0pt,partopsep=0pt}
% %\captionsetup[table]{skip=1pt}
\setlength{\algomargin}{.3cm}

\copyrightyear{2025} 
\acmYear{2025} 
\setcopyright{rightsretained}
\acmConference[ICS '25]{2025 International Conference on Supercomputing}{June 8--11, 2025}{Salt Lake City, UT, USA}
\acmBooktitle{2025 International Conference on Supercomputing (ICS '25), June 8--11, 2025, Salt Lake City, UT, USA}
\acmDOI{10.1145/3721145.3725773}
\acmISBN{979-8-4007-1537-2/2025/06}

%\settopmatter{printfolios=false}
%\settopmatter{printacmref=false}


\begin{document}

\title{MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix Multiplication on CPUs}

\author{Jordi Wolfson-Pou}
\affiliation{
  \institution{Intel Labs}
  \city{Santa Clara}
  \state{California}
  \country{USA}
}
\email{jordi.wolfson-pou@intel.com}

\author{Jan Laukemann}
\affiliation{
  %\institution{Friedrich-Alexander-Universit{\"a}t Erlangen-N{\"u}rnberg, Erlangen National High Performance Computing Center}
  \institution{Friedrich-Alexander-Universit{\"a}t Erlangen-N{\"u}rnberg, Erlangen National High Performance Computing Center}
  \city{Erlangen}
  \country{Germany}
}
\email{jan.laukemann@fau.de}


\author{Fabrizio Petrini}
\affiliation{
  \institution{Intel Labs}
  \city{Santa Clara}
  \state{California}
  \country{USA}
}
\email{fabrizio.petrini@intel.com}

\begin{abstract}
Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation in many applications.  Current multithreaded implementations are based on Gustavsonâ€™s algorithm and often perform poorly on large matrices due to limited cache reuse by the accumulators.  We present MAGNUS (Matrix Algebra for Gigantic NUmerical Systems), a novel algorithm to maximize data locality in SpGEMM.  To generate locality, MAGNUS reorders the intermediate product into discrete cache-friendly chunks using a two-level hierarchical approach.  The accumulator is applied to each chunk, where the chunk size is chosen such that the accumulator is cache-efficient.  MAGNUS is input- and system-aware: based on the matrix characteristics and target system specifications, the optimal number of chunks is computed by minimizing the storage cost of the necessary data structures.  MAGNUS allows for a hybrid accumulation strategy in which each chunk uses a different accumulator based on an input threshold.  We consider two accumulators: an AVX-512 vectorized bitonic sorting algorithm and classical dense accumulation.  An OpenMP implementation of MAGNUS is compared with several baselines, including Intel MKL, for a variety of different matrices on three Intel architectures.  For matrices from the SuiteSparse collection, MAGNUS is faster than all the baselines in most cases and is often an order of magnitude faster than at least one baseline.  For massive random matrices, MAGNUS scales to the largest matrix sizes, while the baselines do not.  Furthermore, MAGNUS is close to the optimal bound for these matrices, regardless of the matrix size, structure, and density.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002950.10003705.10011686</concept_id>
       <concept_desc>Mathematics of computing~Mathematical software performance</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010169.10010170.10010171</concept_id>
       <concept_desc>Computing methodologies~Shared memory algorithms</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010148.10010149.10010158</concept_id>
       <concept_desc>Computing methodologies~Linear algebra algorithms</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Mathematics of computing~Mathematical software performance}
\ccsdesc[500]{Computing methodologies~Shared memory algorithms}
\ccsdesc[500]{Computing methodologies~Linear algebra algorithms}

\keywords{SpGEMM, Sparse matrices, Multicore CPUs}



\maketitle


\input{intro}
\input{background}
\input{magnus}
\input{results}
\input{conclusion}

%\clearpage
\bibliographystyle{plain}
\bibliography{refs}

\end{document}